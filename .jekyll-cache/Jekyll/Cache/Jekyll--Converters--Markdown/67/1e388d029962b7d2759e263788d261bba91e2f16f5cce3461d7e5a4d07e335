I"|<h2 id="对数几率回归-logitstic-regression">对数几率回归 Logitstic Regression</h2>

<p>考虑二分类任务，将线性回归模型的预测值$h(\boldsymbol x) =\boldsymbol{w}^T\boldsymbol{x} + b$转换为0/1值</p>

<h4 id="对数几率函数-sigmoid">对数几率函数 Sigmoid</h4>

<script type="math/tex; mode=display">y = \frac{1}{1+e^{-z}}</script>

<p><img src="../img/Sigmoid.png" style="zoom:38%;" /></p>

<p>当$z&gt;0$时，$Sigmoid&gt;0$，且$z$越大，$Sigmoid$越接近$1$；$z&lt;0$时，$Sigmoid&lt;0$，$z$越小，$Sigmoid$越接近$0$。</p>

<p>将$h(\boldsymbol x)$代入，可将预测值转换为0/1值，得</p>

<script type="math/tex; mode=display">\hat y = \frac{1}{1+e^{-(\boldsymbol{w}^T\boldsymbol{x}+b)}}</script>

<p>线性模型可表示为</p>

<script type="math/tex; mode=display">h =\boldsymbol{w}^T\boldsymbol{x} + b = \ln \frac{y}{1-y}</script>

<table>
  <tbody>
    <tr>
      <td>设$y$为样本$\boldsymbol{x}$为正例的可能性，即后验概率估计$p(y=1{</td>
      <td>}\boldsymbol{x})$，$1-y$为反例的可能性</td>
    </tr>
  </tbody>
</table>

<script type="math/tex; mode=display">\hat y = \frac{1}{1+e^{-(\boldsymbol{w}^T\boldsymbol{x}+b)}}</script>

<script type="math/tex; mode=display">\ln \frac{y}{1-y}=\boldsymbol{w}^T\boldsymbol{x}+b = \ln \frac{p(y=1|\boldsymbol{x})}{p(y=0|\boldsymbol{x})}</script>

<p>其中$\frac{y}{1-y}$为<strong>几率</strong>odds，$\ln \frac{y}{1-y}$为<strong>对数几率</strong>logit</p>

<p>概率分布如下</p>

<script type="math/tex; mode=display">p(y=1|\boldsymbol{x}) = \frac{e^{\boldsymbol{w}^T\boldsymbol{x}+b}}{1+e^{\boldsymbol{w}^T\boldsymbol{x}+b}}\\
p(y=0|\boldsymbol{x}) = \frac{1}{1+e^{\boldsymbol{w}^T\boldsymbol{x}+b}}</script>

<p>可通过<strong>极大似然法</strong>估计$\boldsymbol{w}$和$b$，设数据集为${(\boldsymbol{x_i},y_i)}_{i-1}^m$</p>

<script type="math/tex; mode=display">\ln L(\boldsymbol{w},b)=\sum _{i=1}^m \ln p(y_i|\boldsymbol{x}_i;\boldsymbol{w},b)</script>

<h4 id="代价函数-cost-function">代价函数 Cost Function</h4>

<p>由于的$Sigmoid$函数使之前使用的代价函数成为非凸函数，定义新代价函数为</p>

<script type="math/tex; mode=display">Cost(\hat y (x),y) = 
\left\{
 \begin{array}{**}
-\log(\hat y) , y = 1 \\
-\log(1-\hat y) , y = 0
\end{array}
\right. 
 = -y\log(\hat y)-(1-y)\log(1-\hat y)</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}{}
Loss& =\frac{1}{m}\sum_{i=1}^mCost(\hat y ^{(i)},y^{(i)}) \\
& = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log\hat y ^{(i)}+(1-y^{(i)})\log(1-\hat y ^{(i)}) ]
\end{align} %]]></script>

<p>$Loss$的导数化简结果如下：
<script type="math/tex">Loss(\boldsymbol \theta)= -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log\frac{1}{1+e^{-\boldsymbol{\theta}^T\boldsymbol{x}}}+(1-y^{(i)})\log(1-\frac{1}{1+e^{-\boldsymbol{\theta}^T\boldsymbol{x}}} ]</script></p>

<script type="math/tex; mode=display">\frac{\partial Loss}{\partial \boldsymbol \theta_j} = 
\frac{1}{m}\sum_{i=1}^m(\hat y^{(i)}-y^{(i)})x_j^{(i)}</script>

<h4 id="梯度下降-gradient-descent">梯度下降 Gradient Descent</h4>

<p>如果$Loss(\boldsymbol w )$如果在$\boldsymbol w_i$点可微且有定义，梯度 $\nabla Loss(\boldsymbol w _i)$为函数增长最快的方向，因此梯度的反方向$-\nabla Loss(\boldsymbol w _i)$为函数下降最快的方向。设$\alpha$为学习率，可以用以下公式进行迭代：
<script type="math/tex">% <![CDATA[
\begin{align}{}
\boldsymbol w _{i+1}& = \boldsymbol w _i − α \nabla Loss(\boldsymbol w _i)\\
& = \boldsymbol w _i − α \frac{\partial}{\partial\boldsymbol w _i}Loss(\boldsymbol w) \\
& = \boldsymbol w _i − α \frac{1}{m} \sum_{j=1}^m (\hat y^{(j)}-y^{(j)})x_i^{(j)} 
\end{align} %]]></script></p>

<h4 id="正则化">正则化</h4>

<p>向代价函数中加入L2正则项
<script type="math/tex">Loss =\frac{1}{m}\sum_{i=1}^m [Cost(\hat y (x)^{(i)},y^{(i)})+\lambda \sum \limits_{j=1}^n \boldsymbol w^2_j]</script>
梯度下降法迭代公式为
<script type="math/tex">\boldsymbol w _{i+1}= \boldsymbol w _i − α \frac{1}{m} \sum_{j=1}^m (\hat y(x^{(j)})-y^{(j)})x_i^{(j)} + \frac{\lambda}{m}\boldsymbol w _i</script></p>

<h4 id="优化目标">优化目标</h4>

<script type="math/tex; mode=display">\min_\theta \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}\log(h_{\theta}(x^{(i)}))+(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script>

:ET