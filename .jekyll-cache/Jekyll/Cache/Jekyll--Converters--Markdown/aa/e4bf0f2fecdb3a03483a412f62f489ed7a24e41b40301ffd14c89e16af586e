I")<h3 id="logistic代价函数推导svm">Logistic代价函数推导SVM</h3>

<p>0/1损失函数$l_{0/1}$非凸、非连续，数学性质差，故常用其他函数代替$l_{0/1}$</p>

<p>三种常用的替代损失函数：
<script type="math/tex">l_{hinge}(z) = \max(0,1-z)</script></p>

<script type="math/tex; mode=display">l_{exp}(z) = e^{-z}</script>

<script type="math/tex; mode=display">l_{log}(z) = log(1+e^{-z})</script>

<h4 id="logistic-regression">Logistic Regression</h4>

<script type="math/tex; mode=display">\min_\theta \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}\left[-\log(h_{\theta}(x^{(i)}))\right]+(1-y^{(i)})\left[-\log(1-h_{\theta}(x^{(i)}))\right]\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script>

<script type="math/tex; mode=display">\min_\theta  \left[ \sum_{i=1}^m 
y^{(i)}cost_1
+(1-y^{(i)})cost_0\right] 
+\frac{\lambda}{2}\sum_{j=1}^n\theta_j^2</script>

<script type="math/tex; mode=display">A + \lambda B</script>

<h4 id="svm">SVM</h4>

<script type="math/tex; mode=display">C A + B</script>

<script type="math/tex; mode=display">\min_\theta C  \sum_{i=1}^m\left[ y^{(i)}cost_1(\boldsymbol \theta^T\boldsymbol x^{(i)})+(1-y^{(i)})cost_0({\boldsymbol \theta}^T\boldsymbol x^{(i)})\right]+\frac{1}{2}\sum_{j=1}^n\theta_j^2</script>

<p>其中SVM中的代价函数$cost$采用$hinge$损失
<script type="math/tex">l_{hinge}(z) = \max(0,1-z)</script>
如下所示：</p>

<p><img src="../img/svm_cost.png" alt="5" style="zoom:70%;" /></p>

<p>可以直观地发现当$y=1$时，我们希望$\boldsymbol \theta^T\boldsymbol x \gg 1$，这样使$cost_1$最小；当$y=0$时，我们希望$\boldsymbol \theta^T\boldsymbol x \ll -1$，这样使$cost_0$最小。</p>

<hr />

<h3 id="margin">Margin</h3>

<p>设划分超平面的方程为
<script type="math/tex">\boldsymbol w^T\boldsymbol x +b =0</script></p>

<p>样本中任意点到该超平面的距离为
<script type="math/tex">d = \frac{\boldsymbol w^T x +b}{||\boldsymbol w||}</script>
对于$y=1$，$y=-1$两类样本点，任意点到超平面的距离应大于$d$
<script type="math/tex">\frac{\boldsymbol w^T  x^{(i)} +b}{||\boldsymbol w||} \geq d, \ \forall y^{(i)} = 1\\
\frac{\boldsymbol w^T  x^{(i)} +b}{||\boldsymbol w||} \leq -d, \ \forall y^{(i)} = -1
\\ \Downarrow \\
\frac{\boldsymbol w^T  x^{(i)} +b}{||\boldsymbol w||d} \geq 1, \ \forall y^{(i)} = 1\\
\frac{\boldsymbol w^T  x^{(i)} +b}{||\boldsymbol w||d} \leq -1, \ \forall y^{(i)} = -1
\\ \Downarrow \\
\boldsymbol w_d^T  x^{(i)} +b_d \geq 1, \ \forall y^{(i)} = 1\\
\boldsymbol w_d^T  x^{(i)} +b_d \leq -1, \ \forall y^{(i)} = -1
\\ \Downarrow \\
y^{(i)}(\boldsymbol w^T  x^{(i)} +b) \geq 1</script>
<img src="../img/svm_margin.png" style="zoom:30%;" /></p>

<p>对于任意支撑向量，优化目标为最大化点到超平面的距离：</p>

<script type="math/tex; mode=display">\max \frac{|\boldsymbol w^T x +b|}{||\boldsymbol w||} \Rightarrow \max \frac{1}{||\boldsymbol w||}  \Rightarrow \min \frac{1}{2}||w||^2</script>

<p>则SVM的优化目标为：</p>

<script type="math/tex; mode=display">\min_{\boldsymbol w,b} \frac{1}{2}||\boldsymbol w||^2 \\
{\rm{s.t.}} \ \ y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b)\geq 1, \ i = 1,2,\dots,m.</script>

<hr />

<h3 id="软间隔-soft-margin-svm">软间隔 Soft Margin SVM</h3>

<p>非线性可分情况，允许误差$\xi_i$</p>

<p>L1正则</p>

<script type="math/tex; mode=display">\min_{\boldsymbol w,b} \frac{1}{2}||\boldsymbol w||^2 + C\sum_{i=1}^m \xi_i\\
{\rm{s.t.}} \ \ y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b)\geq 1 - \xi_i \\\ i = 1,2,\dots,m,\ \xi_i\geq0</script>

<p>L2正则</p>

<script type="math/tex; mode=display">\min_{\boldsymbol w,b} \frac{1}{2}||\boldsymbol w||^2 + C\sum_{i=1}^m \xi_i^2\\
{\rm{s.t.}} \ \ y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b)\geq 1 - \xi_i \\\ i = 1,2,\dots,m,\ \xi_i\geq0</script>

<hr />

<h3 id="拉格朗日对偶性-lagrangian-duality">拉格朗日对偶性 Lagrangian Duality</h3>

<p>​	在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转为对偶问题，通过解决对偶问题而得到原始问题的解。当满足一定条件时，原始问题与对偶问题的解是完全等价的。</p>

<p>原始目标：</p>

<script type="math/tex; mode=display">\min_{\boldsymbol w,b} \frac{1}{2}||\boldsymbol w||^2</script>

<script type="math/tex; mode=display">{\rm{s.t.}} \ \ y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b)\geq 1, \ i = 1,2,\dots,m.</script>

<p>使用拉格朗日乘法构造拉格朗日函数转化为无约束优化问题，其中拉格朗日乘子$\alpha_i\geq0$</p>

<p><script type="math/tex">L(\boldsymbol w,b, \boldsymbol \alpha) = \frac{1}{2}||\boldsymbol w||^2 + \sum_{i=1}^m\alpha_i(1-y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b))</script>
将$L(\boldsymbol w,b, \boldsymbol \alpha) $看作是关于$\boldsymbol \alpha$的函数，求</p>

<p><script type="math/tex">\max_{\boldsymbol \alpha} L(\boldsymbol w,b, \boldsymbol \alpha)</script>
当某个约束条件不满足时，如$y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b)&lt; 1$，那么显然有$\max\limits_{\boldsymbol \alpha} L(\boldsymbol w,b, \boldsymbol \alpha) = \infty$（令$\alpha_i=\infty$即可）而当所有约束条件都满足时，则最优值为$\frac{1}{2}||\boldsymbol w||^2$，即最初要最小化的目标值。因此，在约束条件得到满足的情况下最小化$\frac{1}{2}||\boldsymbol w||^2$，等同于求$\max\limits_{\boldsymbol \alpha} L(\boldsymbol w,b, \boldsymbol \alpha)$的最小值。</p>

<p>原始目标变为</p>

<p><script type="math/tex">\min_{\boldsymbol w,b}\max_{\boldsymbol \alpha} L(\boldsymbol w,b, \boldsymbol \alpha)</script>
这个问题只有在满足约束条件下才有极小值，也就是说这个问题的极小值一定满足约束条件。</p>

<p>令$\nabla_\boldsymbol w L = 0$，$\nabla_b L = 0$，得：</p>

<script type="math/tex; mode=display">\boldsymbol w =\sum_{i=1}^m\alpha_iy_i\boldsymbol x_i</script>

<script type="math/tex; mode=display">\sum_{i=1}^m\alpha_iy_i = 0</script>

<p>将其带入，则原始问题转化对偶问题：</p>

<script type="math/tex; mode=display">\max_\boldsymbol \alpha\sum_{i=1}^m \alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol x_i\boldsymbol x_j \\</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
{\rm{s.t.}} &\  \sum_{i=1}^m\alpha_iy_i = 0,\\ 
&\ \alpha_i\geq0, \ i = 1,2,\dots,m
\end{align} %]]></script>

<h4 id="强对偶和弱对偶-strong-and-weak-duality">强对偶和弱对偶 strong and weak duality</h4>

<p>原始问题 Primal Problem</p>

<p><script type="math/tex">p^* = \min_w \max_{\alpha_i \geq 0} L(\boldsymbol w,b, \boldsymbol \alpha)</script>
对偶问题 Dual Problem</p>

<p><script type="math/tex">d^* = \max_{\alpha_i \geq 0}\min_w  L(\boldsymbol w,b, \boldsymbol \alpha)</script>
弱对偶性：
<script type="math/tex">d^* \leq p^*</script>
强对偶性：
<script type="math/tex">d^* = p^*</script></p>

<h4 id="kkt-conditions">KKT conditions</h4>

<ul>
  <li>定常方程式 stationary equation</li>
</ul>

<script type="math/tex; mode=display">\nabla_\boldsymbol w L = 0</script>

<script type="math/tex; mode=display">\nabla_b L = 0</script>

<ul>
  <li>原始可行性 primal feasibility</li>
</ul>

<script type="math/tex; mode=display">y^{(i)}(\boldsymbol w^T \boldsymbol x^{(i)}+b)\geq 1</script>

<ul>
  <li>对偶可行性 dual feasibility</li>
</ul>

<script type="math/tex; mode=display">\alpha_i \geq 0</script>

<ul>
  <li>辅松弛 complementary slackness</li>
</ul>

<script type="math/tex; mode=display">\alpha_i(y^{(i)}f(x^{(i)})-1)=0</script>

<hr />

<h3 id="核函数-kernel-trick">核函数 Kernel Trick</h3>

<p>在<strong>线性不可分</strong>的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开——依靠升维使原本线性不可分的数据线性可分</p>

<p>SVM优化问题经过对偶转化为了
<script type="math/tex">\max_\boldsymbol \alpha\sum_{i=1}^m \alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol x_i\boldsymbol x_j</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
{\rm{s.t.}} &\  \sum_{i=1}^m\alpha_iy_i = 0,\\ 
&\ \alpha_i\geq0, \ i = 1,2,\dots,m
\end{align} %]]></script>

<p>可以看到数据点仅出现为为内积$\boldsymbol x_i\boldsymbol x_j$</p>

<p>只要能在特征空间中计算出内积，就不需要显式的映射。而且许多常见的几何操作（角度、距离）可以用内积表示。减少计算量和储存空间。</p>

<p>定义核函数${\rm{K}} (\boldsymbol x_i,\boldsymbol x_j)=\boldsymbol x_i’·\boldsymbol x_j’=(\boldsymbol x_i·\boldsymbol x_j+1)^2$</p>

<p>核函数本质是将数据添加多项式特征 $x \rightarrow x’$
<script type="math/tex">x'= (x_n^2,\dots,x_1^2,\sqrt2 x_nx_{n-1},\dots,\sqrt2x_n\dots,\sqrt2 x_1,1)</script></p>

<ul>
  <li>核函数不是SVM专有</li>
  <li>存在$\boldsymbol x_i·\boldsymbol x_j$即可应用核函数</li>
</ul>

<h4 id="核函数类型">核函数类型</h4>

<ol>
  <li>线性核函数 <em>Linear kernel</em></li>
</ol>

<script type="math/tex; mode=display">{\rm{K}} (\boldsymbol x_i,\boldsymbol x_j)=\boldsymbol x_i·\boldsymbol x_j</script>

<ol>
  <li>多项式核函数 <em>Polynomial kernel</em></li>
</ol>

<script type="math/tex; mode=display">{\rm{K}} (\boldsymbol x_i,\boldsymbol x_j)=(\boldsymbol x_i·\boldsymbol x_j+1)^d</script>

<ol>
  <li>高斯核函数 /  径向基函数 Radial Basis Function Kernel —— RBF</li>
</ol>

<script type="math/tex; mode=display">{\rm{K}} (\boldsymbol x_i,\boldsymbol x_j)=e^{-\gamma||\boldsymbol x_i-\boldsymbol x_j||^2}</script>

<p>将每个样本点映射到无穷维的特征空间，每个数据点都是landmark
<script type="math/tex">x \rightarrow (e^{-\gamma||\boldsymbol x-\boldsymbol l_1||^2},e^{-\gamma||\boldsymbol x-\boldsymbol l_2||^2})</script></p>

<script type="math/tex; mode=display">m \times n \rightarrow m \times m</script>

<p>原本$n$维的数据升为$m$维（$m$为数据量）</p>

<p>应用：自然语言处理</p>

<hr />

<h3 id="svm思想解决回归问题">SVM思想解决回归问题</h3>

<p>核心思想：在margin里包含的样本点越多越好</p>

<p>超参数$\epsilon$指定margin到中间直线的距离</p>
:ET