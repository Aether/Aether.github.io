---
layout: post
title: "Crawler"
subtitle: ""
date: 2018-01-30
author: Aether
category: coding
tags: code Python
finished: true
---

## Requests Library

### Seven main methods
- requests.request()
- requests.get()
- requests.head()
- requests.post()
- requests.put()
- requests.patch()
- requests.delete()

~~~
import requests
r = requests.get("http://www.baidu.com")
r.status_code
#200 (Succeed) 404 (Failed)
R.encoding = 'utf-8'
r.text
~~~
    
### request method
    
    requests.request(method, url, **kwargs)
    
method:GET, HEAD, POST, PUT, PATCH, DELETE, OPTION

**kwargs:params data json headers cookies auth files timeout proxies allow_redirects stream verify cert(True/False)
    
### Response Object Properties
- r.status_code
- r.text
- r.encoding ( If charset didn't apparent in header,default:ISO-8859-1)
- r.apparent_encoding
- r.content

### Code Framework
##### Exceptions in Requests Library
- requests.ConnectionError
- requests.HTTPError
- requests.URLRequired
- requests.TooManyRedirects
- requests.ConnectTimeout
- requests.Timeout


- r.raise_for_status 
   ( If return value isn't 200,it will throw requests.HTTPError exception)

~~~
import requests
def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
~~~

### HTTP Protocol

URL: http://host[:port][path]

host:legal Internet host domain name or IP address

port:port number, defalt 80

http://www.hit.edu.cn

http://220.181.111.188/duty

HTTP protocol operation :GET, HEAD, POST, PUT, PATCH, DELETE

    payload{'key1':'value1', 'key2','value2'}
    r = requests.post('http://httpbin.org/post', data = payload)
    print(r.text)
    
### Robot Protocol
User-agent: *

Disallow: /

### Crawl Web pictures
    
    import requests
    import os
    
    url = "http://news.hit.edu.cn/_upload/article/images/8e/ea/8376999048edb1b3ca17cce1e5f2/78f2ba13-d001-4546-b4bf-77f43caf8c6f.jpg"
    root = "Users/ucla/"
    path = root + url.split('/')[-1]
   
    try:
        if not os.path.exists(root):
            os.mkdir(root)
        if not os.path.get(url):
            r = requests.get(url)
            with open(path,'wb') as f:
                f.write(r.content)
                f.close()
                print("文件保存成功")
        else:
            print("文件已存在")
    except:
        print("爬取失败")
        

#### get method

    r = requests.get(url) 
    #return Response 
    requests.get(url,params=None,**kwargs)
    
#### Modify User-agent:

    kv = {'user-agent':'Mozilla/5.0'}
    url = 'https://www.amazon.cn/gp/product/B01M8L5Z3Y'
    r = requestts.get(url, headers = kv)

#### Use Search engine:

http://www.baidu.com/s?wd=keyword

http://www.so.com/s?q=keyword


    import requests
    kv = {'wd':'Python'}
    r = requests.get("http://www.baidu.com/s", params = kv)
    r.request.url  #'http://www.baidu.com/s?wd=Python'
    
#### IP address Searching

    import requests
    url = "http://m.ip138.com/ip.asp?ip="
    r = requests.get(url + '202.204.80.112')
    
## Beautiful Soup

    import requests
    r = requests.get("http://python123.io/ws/demo.html")
    demo = r.text
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(demo, "html.parser")
    soup2 = BeautifulSoup(open("Users/ucla/demo.html"), "html.parser")
    print(soup.prettify())
    
### Tag

    <html>
        <body>
            <p class="titile">...</p>
        <body>
    <html>
    
Basic Element:
Tag, Name, Attributes, NavigableString, Comment.

    from bs4 import BeautifulSoup
    soup = BeautifulSoup(demo, "html.parser")
    soup.a.name 
    soup.a.parent.name
    soup.a.parent.parent.name 
    tag = soup.a
    tag.attrs['class']
    type(tag.attrs) #<class 'dict'>
    
### Traversal
- .contents
- .children
- .descendants
- .parent
- .parents
- .next_sibling
- .previous_sibling
- .next_siblings
- .previous_siblings


    for sibling in soup.a.next_siblings:
        print(sibling)
        
    for sibling in soup.a.previous_siblings:
        print(sibling)
        
    for child in soup.body.children:
        print(child)
        
    for child in soup.body.children:
        print(child)
        
###  Markup Language
#### XML
    
    <name> ... </name>
    <name ... />
    <!-- -->

#### YAML

    key : value
    key : #Comment
    -value1
    -value2
    key :
        subkey : subvalue
    text : |
    bundle data
    
#### JSON

    "key" : "value"
    "key" : ["value1", "value2"]
    "key" :{
        "subkey":"subvalue"
    }

### Process Information

    from bs4 import BeautifulSoup
    soup = BeautifulSoup(demo, "http.parser")
    for link in soup.find_all('a')
        print(link.get('href'))
        
#### find_all
<>.find_all(name,attrs,recursive,string,**kwargs)

    for tag in soup.find_all(True):
        print(tag)
        
    import re
    for tag in soup.find_all(re.compile('b')):
        print(tag.name)

attrs:

    soup.find_all('p', 'course')
    soup.findall('id=link1')
    import re
    soup.find_all(id=re.compile('link'))
    
recursive:

    soup.find_all('a')
    soup.find_all('a',recursive=False)
    
string:

    soup.find_all(string = "Basic Python")
    soup.find_all(string = re.compile("python"))
    
Extension Methods

    soup() #soup.find_all(...) 
    
- <>.find()
- <>.find_parents()
- <>.find_parent()
- <>.find_next_siblings()
- <>.find_next_sibling()
- <>.find_previous_siblings()
- <>.find_previous_sibling()

## RE

### Grammer
- [abc] [a-z] [^abc] 
- abc*  ab  abc  abc  abccc...
- abc+ abc abcc abccc...
- abc? ab abc
- abc\|def  abc def
- ab{2}c abbc
- ab{1,2}c abc abbc
- ^abc  abclskejg
- abc$  sldkjgabc
- \d  [0-9]
- \w  [A-Za-z0-9_]


### Example
- \^[A-Za-z]+$
- \^[A-Za-z0-9]+$
- \^-?\d+$ 
- \^[0-9]\*[1-9][0-9]*$
- [1-9]\d{5}
- [\u4e00-\u9fa5]   judge character in utf-8
- (([1-9]?\d\|1\d{2}\|2[0-4]\d\|25[0-5]).){3}[1-9]?\d\|1\d{2}\|2[0-4]\d\|25[0-5]  IP address

### Main Functions
- re.search() return match object

re.search(pattern, string, flags=0)

flags:re.I re.IGNORECASE, re.M re.MULTILINE, re.S re.DOTALL
- re.match() return match object
- re.findall()
- re.split() return list

re.split(pattern, string, maxsplit=0, flags=0) 

- re.finditer() return match object
- re.sub()

re.sub(pattern, repl, string, count=0, flags=0)

match.string
match.re
match.pos
match.endpos

match.group(0)
match.start()
match.end()
match.span()

*?   +?  ??  {m,n}?  return min
